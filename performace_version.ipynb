{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### !!!! DO NOT RUN THIS FIRST CELL UNLESS YOU HAVE THE SAME VENV PATH ISSUE THAT I DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/tunadorable/local-repos/learning_medusa/venv/lib/python3.11/site-packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ok now start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### !!!! ONLY FOR APPLE SILICON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "b = 24 # how many independent sequences will we process in parallel?\n",
    "t = 128 # what is the maximum context length for predictions?\n",
    "max_iters = 10000\n",
    "eval_interval = 100\n",
    "lr = 3e-4\n",
    "eval_iters = 20\n",
    "d = 128\n",
    "h = 8\n",
    "l = 8\n",
    "dropout = 0.2\n",
    "l2 = 0.01\n",
    "\n",
    "m = 5\n",
    "medusa_discount = torch.tensor(0.8).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "v = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - t - m, (b,))\n",
    "    x = torch.stack([data[i:i+t] for i in ix])\n",
    "    y = torch.stack([torch.stack([data[i+1+j:i+t+1+j] for i in ix]) for j in range(m+1)])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss, medusa_logits = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d, 4 * d),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(4 * d, d),\n",
    "            nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(d, head_size, bias=False)\n",
    "        self.query = nn.Linear(d, head_size, bias=False)\n",
    "        self.value = nn.Linear(d, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(t, t)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        b,t,d = x.shape\n",
    "        k = self.key(x)   # (b,t,d/h)\n",
    "        q = self.query(x) # (b,t,d/h)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (b, t, d/h) @ (b, d/h, t) -> (b, t, t)\n",
    "        wei = wei.masked_fill(self.tril[:t, :t] == 0, float('-inf')) # (b, t, t)\n",
    "        wei = F.softmax(wei, dim=-1) # (b, t, t)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (b,t,d/h)\n",
    "        out = wei @ v # (b, t, t) @ (b, t, d/h) -> (b, t, d/h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, h, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(h)])\n",
    "        self.proj = nn.Linear(head_size * h, d)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.proj(torch.cat([head(x) for head in self.heads], dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, d, h):\n",
    "        # d: embedding dimension, h: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = d // h\n",
    "        self.sa = MultiHeadAttention(h, head_size)\n",
    "        self.ffwd = FeedFoward(d)\n",
    "        self.ln1 = nn.LayerNorm(d)\n",
    "        self.ln2 = nn.LayerNorm(d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.ffwd(self.ln2(x + self.sa(self.ln1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class snake(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d,d)\n",
    "        self.relu = nn.ReLU() # actual paper uses SiLU bc they build off Llama\n",
    "        self.w2 = nn.Linear(d,v)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.w2(self.dropout(self.relu(self.w1(x))+x)) # outputs logits shape (b,t,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class medusaGPT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(v, d)\n",
    "        self.position_embedding_table = nn.Embedding(t, d)\n",
    "        self.blocks = nn.Sequential(*[Block(d, h) for _ in range(l)])\n",
    "        self.ln_f = nn.LayerNorm(d) # final layer norm\n",
    "        self.lm_head = nn.Linear(d, v)\n",
    "        \n",
    "        # Create a list of Medusa heads\n",
    "        self.medusa_heads = nn.ModuleList([snake() for _ in range(m)])\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None, medusa_targets=None, verbose=False):\n",
    "        b, t = idx.shape\n",
    "        \n",
    "        pos_emb = self.position_embedding_table(torch.arange(t, device=device)) # (t,d)\n",
    "        x = self.ln_f(self.blocks(pos_emb + self.token_embedding_table(idx)))\n",
    "        \n",
    "        logits = self.lm_head(x) # (b,t,d)@(d,v)=(b,t,v)\n",
    "        \n",
    "        # Apply each snake head to x and store the results\n",
    "        medusa_logits = torch.stack([head(x) for head in self.medusa_heads], dim=0)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "            medusa_loss = None\n",
    "        else:\n",
    "            m, b, t, v = medusa_logits.shape\n",
    "            logits = logits.view(b*t, v)\n",
    "            targets0 = targets[0].view(b*t)\n",
    "            loss = F.cross_entropy(logits, targets0)\n",
    "            \n",
    "            medusa_loss = torch.stack([F.cross_entropy(medusa_logits[i].view(b*t, v), targets[i+1].view(b*t)) * medusa_discount**(i+1) for i in range(m)])\n",
    "            \n",
    "            loss = loss + medusa_loss.sum()\n",
    "\n",
    "        return logits, loss, medusa_logits\n",
    "\n",
    "    def generate_gpt(self, idx, max_new_tokens, temperature=1.0):\n",
    "        # idx is (b, t) array of indices in the current context\n",
    "        #assert temperature >= 0\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, _, __ = self(idx[:, -t:])\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits[:, -1, :] / (temperature+1e-10), dim=-1) # (b, d)\n",
    "            \n",
    "            idx = torch.cat((idx, torch.multinomial(probs, num_samples=1)), dim=1)\n",
    "            \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "medusaGPT(\n",
       "  (token_embedding_table): Embedding(65, 128)\n",
       "  (position_embedding_table): Embedding(128, 128)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (6): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (7): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=128, out_features=65, bias=True)\n",
       "  (medusa_heads): ModuleList(\n",
       "    (0-4): 5 x snake(\n",
       "      (w1): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (relu): ReLU()\n",
       "      (w2): Linear(in_features=128, out_features=65, bias=True)\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming `MyModel` is the class of your model\n",
    "model = medusaGPT().to(device)  # Initialize a model with the same architecture\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load('models/medusa_b24_t128_d128_h8_l8_lr0.0003_drop0.2_l2-0.01_m5_mdiscount0.80_2024-01-25|23-31-12.pth'))\n",
    "# this is the better of the two models i trained\n",
    "# however the extra medusa heads are near useless\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular GPT Inference\n",
    "\n",
    "So in theory this should be the slowest but really it's not much worse bc of how memory inefficient medusa's ugly sisters are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Rome, bounde be as and hownd neep, comal and,-thest my honown go, with tear,\n",
      "Burking me whond mine statherse before, I's granchists man thilder dest arm,\n",
      "As then, and and me is e's blonext and on youty.\n",
      "\n",
      "VOLVOLUMNERCENIUS:\n",
      "Soundes, our met I hellowish\n",
      "CPU times: user 20.6 s, sys: 1.14 s, total: 21.7 s\n",
      "Wall time: 21.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou R\"\n",
    "context_tensor = torch.tensor([encode(input_str)], dtype=torch.long, device=device)\n",
    "output = model.generate_gpt(context_tensor, max_new_tokens=250)\n",
    "print(decode(output[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per second:  11.737089201877934\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens per second: \", 250/21.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medusa's first sister Stheno (the aggressive one)\n",
    "\n",
    "ChatGPT said:\n",
    "Her name translates to \"strength\" or \"forceful\". Stheno was the eldest and most fierce of the sisters, known for her strength and ferocity. \n",
    "\n",
    "So I guess since this generation is strictly doing greedy decoding i'll name it after her"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Stheno(model, idx, max_runs):\n",
    "    assert idx.size(0) == 1, \"idx must be of size (1, t)\"\n",
    "    \n",
    "    logits, loss, mlogits = model(idx[:, -t:])\n",
    "    mlogits = mlogits[...,-1,:].squeeze(dim=1)\n",
    "    \n",
    "    idx_m_prev = torch.argmax(mlogits, dim=-1, keepdim=True).t()\n",
    "    idx_ntp = torch.argmax(logits, dim=-1, keepdim=True).squeeze(dim=2)\n",
    "    idx = torch.cat((idx, idx_ntp[:,-1].unsqueeze(dim=0)), dim=1) \n",
    "\n",
    "    tok_per_inf = [1]\n",
    "    \n",
    "    for _ in range(max_runs-1): \n",
    "        \n",
    "        logits, loss, mlogits = model(torch.cat((idx, idx_m_prev), dim=1)[:, -t:]) \n",
    "        idx_ntp = torch.argmax(logits, dim=-1, keepdim=True).squeeze(dim=2) \n",
    "        \n",
    "        match_tensor = (idx_m_prev == idx_ntp[:,-(m+1):-1]).int()\n",
    "        zero_positions = torch.cat((1 - match_tensor, torch.ones(match_tensor.size(0), 1, dtype=match_tensor.dtype, device=device)), dim=1).argmax(dim=1)\n",
    "        zero_positions[zero_positions >= match_tensor.size(1)] = match_tensor.size(1)\n",
    "        range_tensor = torch.arange(match_tensor.size(1), device=device).unsqueeze(0).expand_as(match_tensor)\n",
    "        mask = range_tensor < zero_positions.unsqueeze(1)\n",
    "        result = (match_tensor * mask).sum(dim=1).item()\n",
    "\n",
    "        tok_per_inf.append(result+1)\n",
    "        \n",
    "        idx_ntp = idx_ntp[:,-1-m+result].unsqueeze(dim=0)\n",
    "        idx = torch.cat((idx, idx_m_prev[:,:result], idx_ntp),dim=1)\n",
    "        \n",
    "        mlogits = mlogits[...,-1-m+result,:].squeeze(dim=1)\n",
    "        idx_m_prev = torch.argmax(mlogits, dim=-1, keepdim=True).t()\n",
    "        \n",
    "    return idx, tok_per_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Rome there there there thank the my honother so thild thinks thinks ther than think the streep think ther thinks ther than there thinks think there than the stand and thonger think ther think ther think the stand thonger th there thinks think ther than the stand and thonger think ther think ther think the stand thonger th there thinks think ther than the stand and thonger think t\n",
      "CPU times: user 18.2 s, sys: 994 ms, total: 19.2 s\n",
      "Wall time: 18.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou R\"\n",
    "context_tensor = torch.tensor([encode(input_str)], dtype=torch.long, device=device)\n",
    "output, tok_per_inf = generate_Stheno(model, context_tensor, max_runs=250)\n",
    "print(decode(output[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per inference:  1.524\n",
      "tokens per second:  20.37433155080214\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens per inference: \", sum(tok_per_inf)/len(tok_per_inf))\n",
    "print(\"tokens per second: \", sum(tok_per_inf)/18.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### even tho Stheno is faster, notice that it's restricted to greedy decoding which means the output quality is lower. If you compare the two passages you'll see that the one above has far less of a problem with repetition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medusa's second sister Euryale (the explorative one)\n",
    "\n",
    "we'll see how this goes\n",
    "\n",
    "\n",
    "\n",
    "ChatGPT said:\n",
    "\n",
    "Her name means \"far-roaming\" in Greek. Euryale was known for her loud crying or bellowing. If your architecture is meant to explore a wide range of possibilities or to \"roam\" extensively through a dataset, the name Euryale might be suitable. Additionally, if your architecture involves a broad or far-reaching search strategy or is notable for 'broadcasting' its findings extensively (analogous to loud crying), Euryale could be an apt choice.\n",
    "\n",
    "The goal here with Euryale is effectively to bring probabalistic decoding back to Stheno. My hope is that this will only require more ram and not result in any added latency, but we'll see. The basic idea is to use topk and then randomly or probability-wise select from candidate sequences instead of greedy decoding. If anything i think the re-incorporation of topk results will *maybe* add a speed increase (although likely not as fast as the attention-based mechanism used in actual Medusa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinations(tensor):\n",
    "    m, k = tensor.shape\n",
    "\n",
    "    mesh_indices = torch.meshgrid([torch.arange(k) for _ in range(m)][::-1], indexing=\"ij\")\n",
    "\n",
    "    combinations = torch.cat([tensor[m-1-i][mesh_indices[i]].unsqueeze(0) for i in range(m)], dim=0)\n",
    "\n",
    "    return combinations.T.reshape(-1, m).flip(dims=[1])\n",
    "\n",
    "def compare(A,B):\n",
    "    i,j,k = A.shape\n",
    "    \n",
    "    match_tensor = (A == B).int() \n",
    "    \n",
    "    padded_tensor = torch.cat((1 - match_tensor, torch.ones((i,j,1), dtype=match_tensor.dtype, device=device)), dim=-1)\n",
    "    \n",
    "    zero_positions = padded_tensor.argmax(dim=-1)\n",
    "    zero_positions[zero_positions >= k] = k\n",
    "    \n",
    "    range_tensor = torch.arange(m, device=device).unsqueeze(0).expand_as(match_tensor)\n",
    "    \n",
    "    mask = range_tensor < zero_positions.unsqueeze(-1)\n",
    "    \n",
    "    return (match_tensor * mask).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Euryale(model, idx, max_runs, k=2):\n",
    "    logits, loss, mlogits = model(idx[:, -t:])\n",
    "    \n",
    "    mlogits = mlogits[...,-1,:].squeeze(dim=1)\n",
    "    \n",
    "    idx_ntp = torch.argmax(logits, dim=-1, keepdim=True).squeeze(dim=2)\n",
    "    idx = torch.cat((idx, idx_ntp[:,-1].unsqueeze(dim=0)), dim=1)\n",
    "    \n",
    "    tok_per_inf = [1]\n",
    "    \n",
    "    for _ in range(max_runs-1): \n",
    "        \n",
    "        idx_m_topk = torch.topk(mlogits, k, dim=-1, largest=True).indices\n",
    "        mcomb = combinations(idx_m_topk) \n",
    "        \n",
    "        idx_rep = idx.repeat(k**m,1) \n",
    "        \n",
    "        logits, loss, mlogits = model(torch.cat((idx_rep, mcomb), dim=1)[:, -t:])\n",
    "        \n",
    "        idx_ntp_topk = torch.topk(logits, k, dim=-1, largest=True).indices \n",
    "        \n",
    "        idx_check = idx_ntp_topk[:,-(m+1):-1,:] \n",
    "        result = compare(mcomb.unsqueeze(0).repeat(k**m,1,1),torch.stack([combinations(idx_check[i]) for i in range(idx_check.shape[0])]))\n",
    "        \n",
    "        max_val = torch.max(result).item()\n",
    "        tok_per_inf.append(max_val+1)\n",
    "        \n",
    "        if random.choice([True, False]):\n",
    "            max_idx_row = torch.max(result,1).indices[0].item() # most likely\n",
    "            max_idx_col = len(result[max_idx_row]) - 1 - torch.argmax(result[max_idx_row].flip(0)).item() # least likely\n",
    "        elif random.choice([True, False]):\n",
    "            max_idx_row = (len(result) - 1 - torch.max(result.flip(dims=[0]),1).indices)[0].item() # least likely\n",
    "            max_idx_col = torch.argmax(result[max_idx_row]).item() # most likely\n",
    "        else:\n",
    "            max_idx_row = torch.max(result,1).indices[0].item() # most likely\n",
    "            max_idx_col = torch.argmax(result[max_idx_row]).item() # most likely\n",
    "        \n",
    "        idx_m = mcomb[max_idx_row, :max_val].unsqueeze(0) # (k^m,m) -> (1,max_val)\n",
    "        \n",
    "        idx_ntp = torch.argmax(logits, dim=-1, keepdim=True).squeeze(dim=2)[max_idx_row, -1-m+max_val].unsqueeze(0).unsqueeze(0)\n",
    "                \n",
    "        idx = torch.cat((idx, idx_m, idx_ntp),dim=1) \n",
    "        \n",
    "        mlogits = mlogits[:,max_idx_row,-1-m+max_val,:].unsqueeze(dim=1).squeeze(dim=1)        \n",
    "            \n",
    "    return idx, tok_per_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Rome there thet  there therefol ,offfort  the me the myself and to the mare  and thonger \n",
      "Th   the  herest and to the  honown ther  thinde \n",
      "The  that rece theter  thatger think  thet thanks theredit ther  thander th thaee\n",
      "Toetend th the  th thingsrer th things  thet than thate though , and to the savow th  the  sorry sad think  and think  the stand thonger th thoee thinks\n",
      "Th the than to  the   thing  of  to een think  ofronder thi eaar then \n",
      "Th   to eeek the   of thieee th the th there\n",
      "CPU times: user 24.4 s, sys: 4.54 s, total: 28.9 s\n",
      "Wall time: 35.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou R\"\n",
    "context_tensor = torch.tensor([encode(input_str)], dtype=torch.long, device=device)\n",
    "output, tok_per_inf = generate_Euryale(model, context_tensor, max_runs=250, k=2)\n",
    "print(decode(output[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per inference:  1.956\n",
      "tokens per second:  13.852691218130312\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens per inference: \", sum(tok_per_inf)/len(tok_per_inf))\n",
    "print(\"tokens per second: \", sum(tok_per_inf)/35.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aight so sometimes when I run this the output looks good and it's fast, but other times the output looks like this and it's not significantly faster than NTP. Lotts leaving things up to chance here. The great thing about real medusa is that it looks more like Stheno in terms of speed and also doesn't suffer from the probability problem of either of the sisters\n",
    "\n",
    "I'd also like to note that there's no use in trying a value for k other than 2\n",
    "\n",
    "And I didn't even use basic speedup methods like KV caching. Idk if i'm right but i think using those might disproportionately help my Euryale over NTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
