{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/tunadorable/local-repos/learning_medusa/venv/lib/python3.11/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "b = 24 # how many independent sequences will we process in parallel?\n",
    "t = 128 # what is the maximum context length for predictions?\n",
    "max_iters = 10000\n",
    "eval_interval = 100\n",
    "lr = 3e-4\n",
    "eval_iters = 20\n",
    "d = 128\n",
    "h = 8\n",
    "l = 8\n",
    "dropout = 0.2\n",
    "l2 = 0.01\n",
    "\n",
    "medusa_headcount = 3\n",
    "medusa_discount = torch.tensor(0.8).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "v = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - t - medusa_headcount, (b,))\n",
    "    x = torch.stack([data[i:i+t] for i in ix])\n",
    "    y = torch.stack([torch.stack([data[i+1+j:i+t+1+j] for i in ix]) for j in range(medusa_headcount+1)])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss, medusa_logits = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d, 4 * d),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(4 * d, d),\n",
    "            nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(d, head_size, bias=False)\n",
    "        self.query = nn.Linear(d, head_size, bias=False)\n",
    "        self.value = nn.Linear(d, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(t, t)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        b,t,d = x.shape\n",
    "        k = self.key(x)   # (b,t,d/h)\n",
    "        q = self.query(x) # (b,t,d/h)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (b, t, d/h) @ (b, d/h, t) -> (b, t, t)\n",
    "        wei = wei.masked_fill(self.tril[:t, :t] == 0, float('-inf')) # (b, t, t)\n",
    "        wei = F.softmax(wei, dim=-1) # (b, t, t)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (b,t,d/h)\n",
    "        out = wei @ v # (b, t, t) @ (b, t, d/h) -> (b, t, d/h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, h, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(h)])\n",
    "        self.proj = nn.Linear(head_size * h, d)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.proj(torch.cat([head(x) for head in self.heads], dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, d, h):\n",
    "        # d: embedding dimension, h: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = d // h\n",
    "        self.sa = MultiHeadAttention(h, head_size)\n",
    "        self.ffwd = FeedFoward(d)\n",
    "        self.ln1 = nn.LayerNorm(d)\n",
    "        self.ln2 = nn.LayerNorm(d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.ffwd(self.ln2(x + self.sa(self.ln1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class snake(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d,d)\n",
    "        self.relu = nn.ReLU() # actual paper uses SiLU bc they build off Llama\n",
    "        self.w2 = nn.Linear(d,v)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.w2(self.dropout(self.relu(self.w1(x))+x)) # outputs logits shape (b,t,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class medusaGPT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(v, d)\n",
    "        self.position_embedding_table = nn.Embedding(t, d)\n",
    "        self.blocks = nn.Sequential(*[Block(d, h) for _ in range(l)])\n",
    "        self.ln_f = nn.LayerNorm(d) # final layer norm\n",
    "        self.lm_head = nn.Linear(d, v)\n",
    "        \n",
    "        # Create a list of Medusa heads\n",
    "        self.medusa_heads = nn.ModuleList([snake() for _ in range(medusa_headcount)])\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None, medusa_targets=None, verbose=False):\n",
    "        b, t = idx.shape\n",
    "        \n",
    "        pos_emb = self.position_embedding_table(torch.arange(t, device=device)) # (t,d)\n",
    "        x = self.ln_f(self.blocks(pos_emb + self.token_embedding_table(idx)))\n",
    "        \n",
    "        logits = self.lm_head(x) # (b,t,d)@(d,v)=(b,t,v)\n",
    "        \n",
    "        # Apply each snake head to x and store the results\n",
    "        medusa_logits = torch.stack([head(x) for head in self.medusa_heads], dim=0)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "            medusa_loss = None\n",
    "        else:\n",
    "            m, b, t, v = medusa_logits.shape\n",
    "            logits = logits.view(b*t, v)\n",
    "            targets0 = targets[0].view(b*t)\n",
    "            loss = F.cross_entropy(logits, targets0)\n",
    "            \n",
    "            medusa_loss = torch.stack([F.cross_entropy(medusa_logits[i].view(b*t, v), targets[i+1].view(b*t)) * medusa_discount**(i+1) for i in range(m)])\n",
    "            \n",
    "            loss = loss + medusa_loss.sum()\n",
    "\n",
    "        return logits, loss, medusa_logits\n",
    "\n",
    "    def generate_gpt(self, idx, max_new_tokens, temperature=1.0):\n",
    "        # idx is (b, t) array of indices in the current context\n",
    "        #assert temperature >= 0\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, _, __ = self(idx[:, -t:])\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits[:, -1, :] / (temperature+1e-10), dim=-1) # (b, d)\n",
    "            \n",
    "            idx = torch.cat((idx, torch.multinomial(probs, num_samples=1)), dim=1)\n",
    "            \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "medusaGPT(\n",
       "  (token_embedding_table): Embedding(65, 128)\n",
       "  (position_embedding_table): Embedding(128, 128)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (6): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (7): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=128, out_features=65, bias=True)\n",
       "  (medusa_heads): ModuleList(\n",
       "    (0-2): 3 x snake(\n",
       "      (w1): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (relu): ReLU()\n",
       "      (w2): Linear(in_features=128, out_features=65, bias=True)\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming `MyModel` is the class of your model\n",
    "model = medusaGPT().to(device)  # Initialize a model with the same architecture\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load('models/medusa_b24_t128_d128_h8_l8_lr0.0003_drop0.2_m3_mdiscount0.8_2024-01-24|17-36-02.pth'))\n",
    "\n",
    "# If you plan to continue training the model, switch to training mode\n",
    "#model.train()\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular GPT Inference\n",
    "\n",
    "for some reason if i try to run this before doing my later functions than the kernel crashes. I recommend restarting with a fresh kernel for those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Romew my moreor thou, I chinh your do your steecy?\n",
      "\n",
      "\n",
      "VOLAMP OWISCigifful CLAURENCENCENTIO:\n",
      "Bow if intern'd inkiltsat thoul thine meed, impe fargelleringmal stren womble blow:\n",
      "I tooes the mine scomes think as I I, wither, noth age dign umbutiner s,\n",
      "Bend belied, touch Clerciant as one ones\n",
      "Of of whe callowal as I plagincheded herself forth us;\n",
      "We is thee timit co yonague,-----stimbaswoul saided\n",
      "Throus tharn bon deepourged theirged hunderuchan of wated aloved;\n",
      "Hearting things thee him the me thouthe\n",
      "CPU times: user 56.6 s, sys: 2.57 s, total: 59.1 s\n",
      "Wall time: 58.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou R\"\n",
    "context_tensor = torch.tensor([encode(input_str)], dtype=torch.long, device=device)\n",
    "output = model.generate_gpt(context_tensor, max_new_tokens=500)\n",
    "print(decode(output[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per second:  8.605851979345955\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens per second: \", 500/58.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medusa's first sister Stheno (the aggressive one)\n",
    "\n",
    "ChatGPT said:\n",
    "Her name translates to \"strength\" or \"forceful\". Stheno was the eldest and most fierce of the sisters, known for her strength and ferocity. \n",
    "\n",
    "So I guess since this generation is strictly doing greedy decoding i'll name it after her"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Stheno(model, idx, max_runs):\n",
    "    assert idx.size(0) == 1, \"idx must be of size (1, t)\"\n",
    "    \n",
    "    logits, loss, mlogits = model(idx[:, -t:])\n",
    "    mlogits = mlogits[...,-1,:].squeeze(dim=1)\n",
    "    \n",
    "    idx_m_prev = torch.argmax(mlogits, dim=-1, keepdim=True).t()\n",
    "    idx_ntp = torch.argmax(logits, dim=-1, keepdim=True).squeeze(dim=2)\n",
    "    idx = torch.cat((idx, idx_ntp[:,-1].unsqueeze(dim=0)), dim=1) \n",
    "\n",
    "    tok_per_inf = [1]\n",
    "    \n",
    "    for _ in range(max_runs-1): \n",
    "        \n",
    "        logits, loss, mlogits = model(torch.cat((idx, idx_m_prev), dim=1)[:, -t:]) \n",
    "        idx_ntp = torch.argmax(logits, dim=-1, keepdim=True).squeeze(dim=2) \n",
    "        \n",
    "        match_tensor = (idx_m_prev == idx_ntp[:,-(medusa_headcount+1):-1]).int()\n",
    "        zero_positions = torch.cat((1 - match_tensor, torch.ones(match_tensor.size(0), 1, dtype=match_tensor.dtype, device=device)), dim=1).argmax(dim=1)\n",
    "        zero_positions[zero_positions >= match_tensor.size(1)] = match_tensor.size(1)\n",
    "        range_tensor = torch.arange(match_tensor.size(1), device=device).unsqueeze(0).expand_as(match_tensor)\n",
    "        mask = range_tensor < zero_positions.unsqueeze(1)\n",
    "        result = (match_tensor * mask).sum(dim=1).item()\n",
    "\n",
    "        tok_per_inf.append(result+1)\n",
    "        \n",
    "        idx_ntp = idx_ntp[:,-1-medusa_headcount+result].unsqueeze(dim=0)\n",
    "        idx = torch.cat((idx, idx_m_prev[:,:result], idx_ntp),dim=1)\n",
    "        \n",
    "        mlogits = mlogits[...,-1-medusa_headcount+result,:].squeeze(dim=1)\n",
    "        idx_m_prev = torch.argmax(mlogits, dim=-1, keepdim=True).t()\n",
    "        \n",
    "    return idx, tok_per_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per inference:  1.392\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Rome this the shoul the shoul the shame the shame the shame of the soul thine one,\n",
      "And thaker thally the that I with this the soul of the soul of the soul things one of this soul the shoul the shoul the shame of the soul things one of this soul the shame of the soul things one of this soul the shame of the soul things one of this soul the shoul th\n",
      "CPU times: user 26.5 s, sys: 1.2 s, total: 27.7 s\n",
      "Wall time: 27.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou R\"\n",
    "context_tensor = torch.tensor([encode(input_str)], dtype=torch.long, device=device)\n",
    "output, tok_per_inf = generate_Stheno(model, context_tensor, max_runs=250)\n",
    "print(\"tokens per inference: \", sum(tok_per_inf)/len(tok_per_inf))\n",
    "print(decode(output[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per second:  12.747252747252746\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens per second: \", sum(tok_per_inf)/27.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### even tho Stheno is ~45% faster, notice that it's restricted to greedy decoding which means the output quality is lower. If you compare the two passages you'll see that the one above has far less of a problem with repetition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medusa's second sister Euryale (the explorative one)\n",
    "\n",
    "we'll see how this goes\n",
    "\n",
    "\n",
    "\n",
    "ChatGPT said:\n",
    "\n",
    "Her name means \"far-roaming\" in Greek. Euryale was known for her loud crying or bellowing. If your architecture is meant to explore a wide range of possibilities or to \"roam\" extensively through a dataset, the name Euryale might be suitable. Additionally, if your architecture involves a broad or far-reaching search strategy or is notable for 'broadcasting' its findings extensively (analogous to loud crying), Euryale could be an apt choice.\n",
    "\n",
    "The goal here with Euryale is effectively to bring probabalistic decoding back to Stheno. My hope is that this will only require more ram and not result in any added latency, but we'll see. The basic idea is to use topk and then randomly or probability-wise select from candidate sequences instead of greedy decoding. If anything i think the re-incorporation of topk results will *maybe* add a speed increase (although likely not as fast as the attention-based mechanism used in actual Medusa)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
